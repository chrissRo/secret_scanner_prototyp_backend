[
 {
  "Description": "Generic API Key",
  "StartLine": 554,
  "EndLine": 555,
  "StartColumn": 33,
  "EndColumn": 1,
  "Match": "client_protocol = TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V8",
  "Secret": "TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V8",
  "File": "sql/hive-thriftserver/if/TCLIService.thrift",
  "SymlinkFile": "",
  "Commit": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4",
  "Entropy": 4.313641,
  "Author": "Davies Liu",
  "Email": "davies@databricks.com",
  "Date": "2016-04-29T16:32:42Z",
  "Message": "[SPARK-14987][SQL] inline hive-service (cli) into sql/hive-thriftserver\n\n## What changes were proposed in this pull request?\n\nThis PR copy the thrift-server from hive-service-1.2 (including  TCLIService.thrift and generated Java source code) into sql/hive-thriftserver, so we can do further cleanup and improvements.\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Davies Liu \u003cdavies@databricks.com\u003e\n\nCloses #12764 from davies/thrift_server.",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4:sql/hive-thriftserver/if/TCLIService.thrift:generic-api-key:554"
 },
 {
  "Description": "Generic API Key",
  "StartLine": 144,
  "EndLine": 144,
  "StartColumn": 11,
  "EndColumn": 109,
  "Match": "client_protocol = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V8;",
  "Secret": "org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V8",
  "File": "sql/hive-thriftserver/src/gen/java/org/apache/hive/service/cli/thrift/TOpenSessionReq.java",
  "SymlinkFile": "",
  "Commit": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4",
  "Entropy": 4.639186,
  "Author": "Davies Liu",
  "Email": "davies@databricks.com",
  "Date": "2016-04-29T16:32:42Z",
  "Message": "[SPARK-14987][SQL] inline hive-service (cli) into sql/hive-thriftserver\n\n## What changes were proposed in this pull request?\n\nThis PR copy the thrift-server from hive-service-1.2 (including  TCLIService.thrift and generated Java source code) into sql/hive-thriftserver, so we can do further cleanup and improvements.\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Davies Liu \u003cdavies@databricks.com\u003e\n\nCloses #12764 from davies/thrift_server.",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4:sql/hive-thriftserver/src/gen/java/org/apache/hive/service/cli/thrift/TOpenSessionReq.java:generic-api-key:144"
 },
 {
  "Description": "Generic API Key",
  "StartLine": 191,
  "EndLine": 191,
  "StartColumn": 11,
  "EndColumn": 109,
  "Match": "client_protocol = org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V8;",
  "Secret": "org.apache.hive.service.cli.thrift.TProtocolVersion.HIVE_CLI_SERVICE_PROTOCOL_V8",
  "File": "sql/hive-thriftserver/src/gen/java/org/apache/hive/service/cli/thrift/TOpenSessionReq.java",
  "SymlinkFile": "",
  "Commit": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4",
  "Entropy": 4.639186,
  "Author": "Davies Liu",
  "Email": "davies@databricks.com",
  "Date": "2016-04-29T16:32:42Z",
  "Message": "[SPARK-14987][SQL] inline hive-service (cli) into sql/hive-thriftserver\n\n## What changes were proposed in this pull request?\n\nThis PR copy the thrift-server from hive-service-1.2 (including  TCLIService.thrift and generated Java source code) into sql/hive-thriftserver, so we can do further cleanup and improvements.\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Davies Liu \u003cdavies@databricks.com\u003e\n\nCloses #12764 from davies/thrift_server.",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4:sql/hive-thriftserver/src/gen/java/org/apache/hive/service/cli/thrift/TOpenSessionReq.java:generic-api-key:191"
 },
 {
  "Description": "Generic API Key",
  "StartLine": 92,
  "EndLine": 92,
  "StartColumn": 35,
  "EndColumn": 73,
  "Match": "CLIENT_TOKEN = \"hiveserver2ClientToken\"",
  "Secret": "hiveserver2ClientToken",
  "File": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/auth/HiveAuthFactory.java",
  "SymlinkFile": "",
  "Commit": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4",
  "Entropy": 3.5680842,
  "Author": "Davies Liu",
  "Email": "davies@databricks.com",
  "Date": "2016-04-29T16:32:42Z",
  "Message": "[SPARK-14987][SQL] inline hive-service (cli) into sql/hive-thriftserver\n\n## What changes were proposed in this pull request?\n\nThis PR copy the thrift-server from hive-service-1.2 (including  TCLIService.thrift and generated Java source code) into sql/hive-thriftserver, so we can do further cleanup and improvements.\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Davies Liu \u003cdavies@databricks.com\u003e\n\nCloses #12764 from davies/thrift_server.",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4:sql/hive-thriftserver/src/main/java/org/apache/hive/service/auth/HiveAuthFactory.java:generic-api-key:92"
 },
 {
  "Description": "Generic API Key",
  "StartLine": 42,
  "EndLine": 42,
  "StartColumn": 34,
  "EndColumn": 72,
  "Match": "TOKEN = \"HiveServer2ImpersonationToken\"",
  "Secret": "HiveServer2ImpersonationToken",
  "File": "sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java",
  "SymlinkFile": "",
  "Commit": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4",
  "Entropy": 3.8278327,
  "Author": "Davies Liu",
  "Email": "davies@databricks.com",
  "Date": "2016-04-29T16:32:42Z",
  "Message": "[SPARK-14987][SQL] inline hive-service (cli) into sql/hive-thriftserver\n\n## What changes were proposed in this pull request?\n\nThis PR copy the thrift-server from hive-service-1.2 (including  TCLIService.thrift and generated Java source code) into sql/hive-thriftserver, so we can do further cleanup and improvements.\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Davies Liu \u003cdavies@databricks.com\u003e\n\nCloses #12764 from davies/thrift_server.",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "7feeb82cb7f462e44f7e698c7c3b6ac3a77aade4:sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java:generic-api-key:42"
 },
 {
  "Description": "Generic API Key",
  "StartLine": 112,
  "EndLine": 113,
  "StartColumn": 13,
  "EndColumn": 1,
  "Match": "ACCESS_KEY_ID=ABCDEFG1234567890123",
  "Secret": "ABCDEFG1234567890123",
  "File": "docs/ec2-scripts.md",
  "SymlinkFile": "",
  "Commit": "d12c0711faa3d4333513fcbbbee4868bcb784a26",
  "Entropy": 4.0219283,
  "Author": "Mike Jennings",
  "Email": "mvj101@gmail.com",
  "Date": "2014-12-16T20:13:21Z",
  "Message": "[SPARK-3405] add subnet-id and vpc-id options to spark_ec2.py\n\nBased on this gist:\nhttps://gist.github.com/amar-analytx/0b62543621e1f246c0a2\n\nWe use security group ids instead of security group to get around this issue:\nhttps://github.com/boto/boto/issues/350\n\nAuthor: Mike Jennings \u003cmvj101@gmail.com\u003e\nAuthor: Mike Jennings \u003cmvj@google.com\u003e\n\nCloses #2872 from mvj101/SPARK-3405 and squashes the following commits:\n\nbe9cb43 [Mike Jennings] `pep8 spark_ec2.py` runs cleanly.\n4dc6756 [Mike Jennings] Remove duplicate comment\n731d94c [Mike Jennings] Update for code review.\nad90a36 [Mike Jennings] Merge branch 'master' of https://github.com/apache/spark into SPARK-3405\n1ebffa1 [Mike Jennings] Merge branch 'master' into SPARK-3405\n52aaeec [Mike Jennings] [SPARK-3405] add subnet-id and vpc-id options to spark_ec2.py",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "d12c0711faa3d4333513fcbbbee4868bcb784a26:docs/ec2-scripts.md:generic-api-key:112"
 },
 {
  "Description": "Generic API Key",
  "StartLine": 56,
  "EndLine": 57,
  "StartColumn": 13,
  "EndColumn": 1,
  "Match": "ACCESS_KEY_ID=ABCDEFG1234567890123",
  "Secret": "ABCDEFG1234567890123",
  "File": "docs/ec2-scripts.md",
  "SymlinkFile": "",
  "Commit": "9e8ced7847d84d63f0da08b15623d558a2407583",
  "Entropy": 4.0219283,
  "Author": "Jeff Steinmetz",
  "Email": "jeffrey.steinmetz@gmail.com",
  "Date": "2014-09-27T06:00:40Z",
  "Message": "stop, start and destroy require the EC2_REGION\n\ni.e\n./spark-ec2 --region=us-west-1 stop yourclustername\n\nAuthor: Jeff Steinmetz \u003cjeffrey.steinmetz@gmail.com\u003e\n\nCloses #2473 from jeffsteinmetz/master and squashes the following commits:\n\n7491f2c [Jeff Steinmetz] fix case in EC2 cluster setup documentation\nbd3d777 [Jeff Steinmetz] standardized ec2 documenation to use \u003clower-case\u003e sample args\n2bf4a57 [Jeff Steinmetz] standardized ec2 documenation to use \u003clower-case\u003e sample args\n68d8372 [Jeff Steinmetz] standardized ec2 documenation to use \u003clower-case\u003e sample args\nd2ab6e2 [Jeff Steinmetz] standardized ec2 documenation to use \u003clower-case\u003e sample args\n520e6dc [Jeff Steinmetz] standardized ec2 documenation to use \u003clower-case\u003e sample args\n37fc876 [Jeff Steinmetz] stop, start and destroy require the EC2_REGION",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "9e8ced7847d84d63f0da08b15623d558a2407583:docs/ec2-scripts.md:generic-api-key:56"
 },
 {
  "Description": "AWS",
  "StartLine": 639,
  "EndLine": 639,
  "StartColumn": 65,
  "EndColumn": 84,
  "Match": "AKIAI2EGAQ7GYNL4LRAA",
  "Secret": "AKIAI2EGAQ7GYNL4LRAA",
  "File": "ec2/spark_ec2.py",
  "SymlinkFile": "",
  "Commit": "642ab5c1e1ba98833265447447702c3c39fb2d40",
  "Entropy": 3.441446,
  "Author": "Reza Zadeh",
  "Email": "rizlar@gmail.com",
  "Date": "2013-12-27T06:51:19Z",
  "Message": "initial large scale testing begin",
  "Tags": [],
  "RuleID": "aws-access-token",
  "Fingerprint": "642ab5c1e1ba98833265447447702c3c39fb2d40:ec2/spark_ec2.py:aws-access-token:639"
 },
 {
  "Description": "Hardcoded credentials in source-code files",
  "StartLine": 447,
  "EndLine": 447,
  "StartColumn": 24,
  "EndColumn": 68,
  "Match": "Token: \" + report.getClientToAMToken() + \"\\n\"",
  "Secret": "Token: \" + report.getClientToAMToken() + \"\\n\"",
  "File": "new-yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala",
  "SymlinkFile": "",
  "Commit": "2d3eae244bc1b5e6a22798cd307fba3026053edc",
  "Entropy": 4.1699877,
  "Author": "Matei Zaharia",
  "Email": "matei@eecs.berkeley.edu",
  "Date": "2013-12-05T07:33:04Z",
  "Message": "Merge pull request #199 from harveyfeng/yarn-2.2\n\nHadoop 2.2 migration\n\nIncludes support for the YARN API stabilized in the Hadoop 2.2 release, and a few style patches.\n\nShort description for each set of commits:\n\na98f5a0 - \"Misc style changes in the 'yarn' package\"\na67ebf4 - \"A few more style fixes in the 'yarn' package\"\nBoth of these are some minor style changes, such as fixing lines over 100 chars, to the existing YARN code.\n\nab8652f - \"Add a 'new-yarn' directory ... \"\nCopies everything from `SPARK_HOME/yarn` to `SPARK_HOME/new-yarn`. No actual code changes here.\n\n4f1c3fa - \"Hadoop 2.2 YARN API migration ...\"\nAPI patches to code in the `SPARK_HOME/new-yarn` directory. There are a few more small style changes mixed in, too.\nBased on @colorant's Hadoop 2.2 support for the scala-2.10 branch in #141.\n\na1a1c62 - \"Add optional Hadoop 2.2 settings in sbt build ... \"\nIf Spark should be built against Hadoop 2.2, then:\na) the `org.apache.spark.deploy.yarn` package will be compiled from the `new-yarn` directory.\nb) Protobuf v2.5 will be used as a Spark dependency, since Hadoop 2.2 depends on it. Also, Spark will be built against a version of Akka v2.0.5 that's built against Protobuf 2.5, named `akka-2.0.5-protobuf-2.5`. The patched Akka is here: https://github.com/harveyfeng/akka/tree/2.0.5-protobuf-2.5, and was published to local Ivy during testing.\n\nThere's also a new boolean environment variable, `SPARK_IS_NEW_HADOOP`, that users can manually set if their `SPARK_HADOOP_VERSION` specification does not start with `2.2`, which is how the build file tries to detect a 2.2 version. Not sure if this is necessary or done in the best way, though...\n(cherry picked from commit 72b696156c8662cae2cef4b943520b4be86148ea)\n\nConflicts:\n\n        project/SparkBuild.scala\n        streaming/pom.xml",
  "Tags": [],
  "RuleID": "hardcoded_credentials",
  "Fingerprint": "2d3eae244bc1b5e6a22798cd307fba3026053edc:new-yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:hardcoded_credentials:447"
 },
 {
  "Description": "Hardcoded credentials in source-code files",
  "StartLine": 426,
  "EndLine": 426,
  "StartColumn": 20,
  "EndColumn": 60,
  "Match": "Token: \" + report.getClientToken() + \"\\n\"",
  "Secret": "Token: \" + report.getClientToken() + \"\\n\"",
  "File": "new-yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala",
  "SymlinkFile": "",
  "Commit": "ab8652f2d3bf4aa28430867a83cad5ca0f9c1091",
  "Entropy": 4.053066,
  "Author": "Harvey Feng",
  "Email": "harvey@databricks.com",
  "Date": "2013-11-21T11:58:08Z",
  "Message": "Add a \"new-yarn\" directory in SPARK_HOME, intended to contain Hadoop-2.2 API changes.",
  "Tags": [],
  "RuleID": "hardcoded_credentials",
  "Fingerprint": "ab8652f2d3bf4aa28430867a83cad5ca0f9c1091:new-yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:hardcoded_credentials:426"
 },
 {
  "Description": "Generic API Key",
  "StartLine": 798,
  "EndLine": 799,
  "StartColumn": 10,
  "EndColumn": 1,
  "Match": "accessPath = myImportsCode._3",
  "Secret": "myImportsCode._3",
  "File": "src/scala/spark/repl/SparkInterpreter.scala",
  "SymlinkFile": "",
  "Commit": "df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9",
  "Entropy": 3.75,
  "Author": "Matei Zaharia",
  "Email": "matei@eecs.berkeley.edu",
  "Date": "2010-03-29T23:17:55Z",
  "Message": "Initial commit",
  "Tags": [],
  "RuleID": "generic-api-key",
  "Fingerprint": "df29d0ea4c8b7137fdd1844219c7d489e3b0d9c9:src/scala/spark/repl/SparkInterpreter.scala:generic-api-key:798"
 }
]
